<h3 align="center">
Multi-Level Triton Runner Tool üîß
</h3>

<p align="center">
<a href="./doc/"><b>Documentation</b></a> ÔΩú <a href="https://triton-runner.org"><b>üîó triton-runner.org</b></a>
</p>

<p align="center">
<a ><b>English</b></a> | <a href="README.zh.md"><b>‰∏≠Êñá</b></a>
</p>

triton-runner is a lightweight, multi-level execution engine for [OpenAI/Triton](https://github.com/triton-lang/triton), designed to support IR/PTX/cubin launches in complex pass pipelines.

triton-runner is compatible with **Triton v3.4.0 (primary), v3.3.x, or v3.2.0**, and may not work with other versions.

## ‚ú® Features

- Support multi-level execution
- TTIR Debug
- Benchmark performance

## üì¶ Installation

### Quick Installation

You can install the latest stable release of Triton from pip.

```shell
pip install triton-runner
```

### Install from source

You can install from source to access the latest features and developments.

```shell
git clone https://github.com/OpenMLIR/triton-runner
cd triton-runner

pip install -e .
```

## üöÄ Quick Start

### multi-level execution 

#### 1. Python runner

You can run your Triton code using `@triton_runner.jit` instead of `@triton.jit`. See an example in [examples/python_runner/matmul.py](https://github.com/OpenMLIR/triton-runner/blob/main/examples/python_runner/matmul.py#L12)

You can run the example with `python examples/python_runner/matmul.py`. After running successfully, you should see output like `[triton-runner] Triton cache saved`.

#### 2. TTIR runner

In addition to using `@triton_runner.jit` instead of `@triton.jit`, you also need to provide the TTIR file. You can place it in the same directory as the current Python file and use `ttir_dir=triton_runner.get_file_dir(__file__)`. See an example in [examples/ttir_runner/matmul.py](https://github.com/OpenMLIR/triton-runner/blob/main/examples/ttir_runner/matmul.py#L67). Alternatively, you can use the Triton cache directory generated by the Python runner(previous step).

You can run the example with `python examples/ttir_runner/matmul.py`. 

#### 3. TTGIR/LLIR/PTX/cubin runner

In addition to using `@triton_runner.jit` instead of `@triton.jit`, you also need to provide the corresponding file. Like the TTGIR runner, You can place it in the same directory as the current Python file and use `ttgir_dir=triton_runner.get_file_dir(__file__)`. Since all of them are architecture-specific, be sure to use the corresponding metadata JSON file. See an example in [examples/ttgir_runner/sm90/matmul-with-tma-v4.py](https://github.com/OpenMLIR/triton-runner/blob/main/examples/ttgir_runner/sm90/matmul-with-tma-v4.py#L76).

If your architecture is `sm90`, you can run the example using the TTGIR runner with `python examples/ttgir_runner/sm90/matmul-with-tma-v4.py`. 

#### 4„ÄÅSummary

To run your Triton code, replace `@triton.jit` with `@triton_runner.jit`.  

I provide examples for different architectures and Triton versions. 

Provide examples for architectures include `sm90 (H100, H200, H20, etc.)`, `sm80 (A100, A30)`, `sm120 (RTX PRO 6000, RTX 5090, etc.)`, `sm86 (A10, RTX 3090, etc.)` or `sm75 (T4, RTX 2080, etc.)`. If your GPU does not have one of the above compute capabilities, you can use `TRITON_CACHE_DIR=$PWD/.cache` to output the Triton cache to the current directory, and then copy the corresponding cache files to your target machine.

Here's an example command that targets sm90 with Triton v3.4.0.

```shell
python examples/python_runner/matmul.py

python examples/ttir_runner/matmul.py

python examples/ttgir_runner/sm90/matmul-with-tma-v4.py

python examples/llir_runner/sm90/matmul-with-tma-v4.py

python examples/ptx_runner/sm90/matmul-with-tma-v4.py

python examples/cubin_runner/sm90/matmul-with-tma-v4.py
```

For other target, please refer to [examples](./doc/examples_v3.4.0.md). If your Triton version is v3.3.1 or v3.3.0, please refer to [examples_v3.3.x](./doc/examples_v3.3.x.md) for example commands. If your Triton version is v3.2.0, please refer to [examples_v3.2.0](./doc/examples_v3.2.0.md) for example commands.

### TTIR Debug(main branch)

Debugging is supported for TTIR ops like `tt.load`, `arith.addf`, and `tt.trans` in Triton v3.4.0.

```shell
python debug_tool/ttir/01-vector_add/debug_load.py
python debug_tool/ttir/01-vector_add/debug_addf.py

python debug_tool/ttir/02-matrix_transpose/debug_2d_load.py
python debug_tool/ttir/02-matrix_transpose/debug_2d_trans.py

python debug_tool/ttir/03-matrix_multiplication/debug_acc.py

python debug_tool/ttir/04-softmax/debug_maxnumf.py
python debug_tool/ttir/04-softmax/debug_addf-sum.py
python debug_tool/ttir/04-softmax/debug_subf.py
python debug_tool/ttir/04-softmax/debug_exp-exp_shifted.py
python debug_tool/ttir/04-softmax/debug_divf-normalize_by_sum.py

python debug_tool/ttir/05-softmax_lse/debug_log_acc.py
python debug_tool/ttir/05-softmax_lse/debug_max_acc.py
python debug_tool/ttir/05-softmax_lse/debug_more.py

python debug_tool/ttir/06-attention/debug_out.py

python debug_tool/ttir/07-debug_not_f32/debug_bf16.py
```

### Benchmarks

Benchmarks Referencing [TritonBench](https://github.com/pytorch-labs/tritonbench)
  - `launch_latency`: Measures kernel launch overhead.
  - `matmul`: Provides a benchmark for matrix multiplication performance.

```shell
python benchmark/launch_latency/bench.py

python benchmark/static_shape/matmul.py
```

## ‚ö†Ô∏è Version Compatibility

triton-runner is compatible with **Triton v3.4.0 (primary), v3.3.x, or v3.2.0**.

Compatibility with other versions of Triton is **not guaranteed** and may lead to unexpected behavior or run failures.

## üìÑ License

This project is licensed under the **MIT License**.
See the [LICENSE](./LICENSE) file for more details.
